{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed13bf24",
   "metadata": {},
   "source": [
    "Precision and recall are two important metrics in the field of machine learning and statistics, particularly in classification problems. They are both derived from the confusion matrix, which is a table used to describe the performance of a classification model on a set of test data for which the true values are known. To understand precision and recall, let's first define the components of a confusion matrix:\n",
    "\n",
    "True Positives (TP): The number of positive instances correctly classified as positive.\n",
    "True Negatives (TN): The number of negative instances correctly classified as negative.\n",
    "False Positives (FP): The number of negative instances incorrectly classified as positive.\n",
    "False Negatives (FN): The number of positive instances incorrectly classified as negative.\n",
    "Precision\n",
    "Definition: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n",
    "Formula: Precision = TP / (TP + FP)\n",
    "Interpretation: It measures how many of the items identified as positive are actually positive. High precision indicates a low rate of false positives.\n",
    "Use Case: Precision is particularly important in scenarios where the cost of a false positive is high. For example, in email spam detection, a false positive (marking a good email as spam) is more problematic than a false negative (letting a spam email go to the inbox).\n",
    "Recall\n",
    "Definition: Recall (also known as sensitivity or true positive rate) is the ratio of correctly predicted positive observations to all observations in the actual class.\n",
    "Formula: Recall = TP / (TP + FN)\n",
    "Interpretation: It measures how many of the actual positive items are picked up by the classifier. High recall indicates a low rate of false negatives.\n",
    "Use Case: Recall is crucial in situations where missing a positive instance is significantly worse than getting a false positive. For example, in medical diagnoses of a serious disease, it's more important to identify all patients with the disease (high recall) than to ensure everyone diagnosed actually has the disease (high precision).\n",
    "Precision vs Recall\n",
    "Trade-off: In many cases, there is a trade-off between precision and recall. Increasing precision typically reduces recall and vice versa.\n",
    "Balancing Act: The choice between prioritizing precision or recall depends on the specific requirements of the task. In some cases, it’s crucial to minimize false positives, while in others, it’s more important to capture as many positives as possible.\n",
    "F1 Score\n",
    "To balance precision and recall, the F1 Score is often used. It is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n",
    "Formula: F1 = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
