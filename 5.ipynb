{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab23373b",
   "metadata": {},
   "source": [
    "A confusion matrix is a specific table layout that allows visualization of the performance of a classification algorithm, typically a supervised learning one. It is especially useful for assessing the performance of a model on a dataset where the true values are known.\n",
    "\n",
    "Structure of a Confusion Matrix\n",
    "In binary classification, the confusion matrix is a 2x2 matrix that compares the actual target values with those predicted by the machine learning model. The matrix is usually arranged as follows:\n",
    "\n",
    "True Positive (TP): The cases in which the model correctly predicts the positive class.\n",
    "True Negative (TN): The cases in which the model correctly predicts the negative class.\n",
    "False Positive (FP): The cases in which the model incorrectly predicts the positive class (also known as Type I error).\n",
    "False Negative (FN): The cases in which the model incorrectly predicts the negative class (also known as Type II error).\n",
    "What a Confusion Matrix Tells You\n",
    "Accuracy: How often the model is correct. Calculated as \n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "/\n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "(TP+TN)/(TP+TN+FP+FN).\n",
    "\n",
    "Precision: The ratio of correctly predicted positive observations to the total predicted positives. Important in situations where FP is a bigger concern than FN. Calculated as \n",
    "�\n",
    "�\n",
    "/\n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "TP/(TP+FP).\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): The ratio of correctly predicted positive observations to all observations in actual class. Crucial in cases where FN is more serious. Calculated as \n",
    "�\n",
    "�\n",
    "/\n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "TP/(TP+FN).\n",
    "\n",
    "F1 Score: The weighted average of Precision and Recall. Useful when you seek a balance between Precision and Recall. Calculated as \n",
    "2\n",
    "∗\n",
    "(\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "∗\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    ")\n",
    "/\n",
    "(\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    ")\n",
    "2∗(Precision∗Recall)/(Precision+Recall).\n",
    "\n",
    "Specificity (True Negative Rate): Measures the proportion of actual negatives that are correctly identified. Calculated as \n",
    "�\n",
    "�\n",
    "/\n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "TN/(TN+FP).\n",
    "\n",
    "Misclassification Rate (Error Rate): Overall, how often the model is wrong. Calculated as \n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "/\n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "(FP+FN)/(TP+TN+FP+FN).\n",
    "\n",
    "Example\n",
    "Consider a medical test to diagnose a disease:\n",
    "\n",
    "TP: The test correctly identifies 80 patients with the disease.\n",
    "TN: The test correctly identifies 90 patients without the disease.\n",
    "FP: The test incorrectly identifies 10 healthy patients as having the disease.\n",
    "FN: The test misses the disease in 20 patients.\n",
    "The confusion matrix helps in understanding not just the overall accuracy (170 correct out of 200 total), but also how well the test is at identifying positive cases (recall), how often a positive result is correct (precision), and the balance between these metrics (F1 score).\n",
    "\n",
    "Importance\n",
    "Detailed Performance: Unlike overall accuracy, a confusion matrix provides a more detailed breakdown of where the model is performing well and where it is failing.\n",
    "Informing Model Improvement: Helps in understanding the types of errors the model is making, which can guide further improvements.\n",
    "Suitability for Imbalanced Datasets: In datasets where one class significantly outnumbers the other, overall accuracy can be misleading. A confusion matrix helps to assess performance in both classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
